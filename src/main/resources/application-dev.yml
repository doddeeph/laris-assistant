spring:
  application:
    name: LarisAssistant

server:
  port: 8080

twilio:
  auth-token: ${TWILIO_AUTH_TOKEN}

openai:
  api:
    key: ${OPENAI_API_KEY}
    chat-completions-uri: ${OPENAI_API_CHAT_COMPLETIONS_URI}
    model: ${OPENAI_API_MODEL}
    max-token: ${OPENAI_API_MAX_TOKEN}
    temperature: ${OPENAI_API_TEMPERATURE}
    role-system-message: ${OPENAI_API_ROLE_SYSTEM_MESSAGE}

flowiseai:
  api:
    prediction:
      uri: ${FLOWISEAI_API_PREDICTION_URI}
      id: ${FLOWISEAI_API_PREDICTION_ID}

logging:
  level:
    reactor:
      netty:
        http:
          client: DEBUG